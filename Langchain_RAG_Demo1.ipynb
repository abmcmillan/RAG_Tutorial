{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Notebook is a basic example of Retrieval Augmented Generation based on the following Langchain example https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/. It was created for an ARLIS RISC Demonstration in June 2024 by Alan McMillan, PhD."
      ],
      "metadata": {
        "id": "nuCQ_TiVmZ3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-openai langchain-community langchain-chroma"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zwSfadSQds88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9z-2XZadjsC"
      },
      "outputs": [],
      "source": [
        "# specify OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Insert_Key_Here'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the WebBaseLoader class from the langchain_community.document_loaders module\n",
        "# This class is used to load documents from a specified web page\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Create an instance of WebBaseLoader\n",
        "# - The URL of the web page from which we want to load the document is provided as an argument\n",
        "loader = WebBaseLoader(\"https://today.umd.edu/umd-awarded-record-setting-research-contract-worth-up-to-500m\")\n",
        "\n",
        "# Load the document from the specified URL\n",
        "# The loaded content is stored in the 'data' variable\n",
        "data = loader.load()\n",
        "\n",
        "# 'data' now contains the text content of the page loaded from the specified URL"
      ],
      "metadata": {
        "id": "04TM5GRGd7JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RecursiveCharacterTextSplitter class from the langchain_text_splitters module\n",
        "# This class is used to split documents into smaller chunks of text\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create an instance of RecursiveCharacterTextSplitter\n",
        "# - chunk_size: the maximum size of each text chunk (here, 1000 characters)\n",
        "# - chunk_overlap: the number of characters that overlap between consecutive chunks (here, 200 characters)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Split the loaded document into chunks of text\n",
        "# The 'data' variable contains the loaded document to be split\n",
        "# The resulting chunks are stored in the 'splits' variable\n",
        "splits = text_splitter.split_documents(data)\n",
        "\n",
        "# 'splits' now contains the text chunks obtained from splitting the loaded document"
      ],
      "metadata": {
        "id": "qiuQxqJYhkQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Chroma class from the langchain_chroma module\n",
        "# This class is used to interact with a vector database\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Import the OpenAIEmbeddings class from the langchain_openai module\n",
        "# This class is used to compute embeddings for the documents\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Create an instance of Chroma and load the document chunks into the vector database\n",
        "# - documents: the chunks of text to be stored in the vector database (here, 'splits')\n",
        "# - embedding: the embedding model used to compute embeddings for the documents (here, 'OpenAIEmbeddings')\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "\n",
        "# Get a retriever from the vectorstore\n",
        "# A retriever is used to search and retrieve relevant documents based on a query\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 'retriever' now provides an interface to search and retrieve documents from the vector database"
      ],
      "metadata": {
        "id": "fq8PGPgVhnHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system prompt for the retrieval-augmented generation (RAG) task\n",
        "# - This prompt instructs the assistant to use the retrieved context to answer questions concisely\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Import the ChatPromptTemplate class from the langchain_core.prompts module\n",
        "# This class is used to create chat-based prompt templates\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a chat prompt template from the specified messages\n",
        "# - The template consists of a system message and a human message\n",
        "# - The system message uses the 'system_prompt' defined earlier\n",
        "# - The human message includes a placeholder '{input}' for the user's question\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 'prompt' now contains the chat prompt template for the RAG task"
      ],
      "metadata": {
        "id": "i_Q1hkawhnTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatOpenAI class from the langchain_openai module\n",
        "# This class is used to define the language model (LLM) for the task\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create an instance of ChatOpenAI\n",
        "# - model: the specific model to use (here, \"gpt-3.5-turbo-0125\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n",
        "# Import the create_stuff_documents_chain function from the langchain.chains.combine_documents module\n",
        "# This function is used to create a chain for combining document chunks and answering questions\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Create a question-answering chain using the LLM and the prompt template\n",
        "# - llm: the language model to use for answering questions\n",
        "# - prompt: the prompt template to guide the LLM in generating answers\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# Import the create_retrieval_chain function from the langchain.chains module\n",
        "# This function is used to create a retrieval-augmented generation (RAG) chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# Create a RAG chain using the retriever and the question-answering chain\n",
        "# - retriever: the retriever for searching and retrieving relevant documents\n",
        "# - question_answer_chain: the chain for combining document chunks and answering questions\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "# 'rag_chain' now contains the retrieval-augmented generation chain for the task"
      ],
      "metadata": {
        "id": "wLCFiNjniSqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a utility function to wrap print and wrap text for Colab\n",
        "\n",
        "# Import the textwrap module from the standard library\n",
        "# textwrap is used to wrap text into paragraphs with a specified width\n",
        "import textwrap\n",
        "\n",
        "# Print the response with text wrapping for better display\n",
        "# - textwrap.fill: Wraps the input text to the specified width (50 characters in this case)\n",
        "def print_wrapped( text, width=50 ):\n",
        "  print(textwrap.fill( text, width=width))"
      ],
      "metadata": {
        "id": "jUXxnMUsVWtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to answer the question directly using the LLM without using retrieval-augmented generation (RAG)\n",
        "\n",
        "# Invoke the language model (LLM) with a direct question\n",
        "# - The LLM is provided with a list of messages in a conversation format\n",
        "# - The first message is a system message setting the context for the assistant\n",
        "# - The second message is a human message asking the specific question\n",
        "no_rag_response = llm.invoke([\n",
        "    (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
        "    (\"human\", \"What did ARLIS announce on 5/28/2024?\"),\n",
        "])\n",
        "\n",
        "# Print the content of the response from the LLM\n",
        "# - 'no_rag_response.content' contains the generated answer from the LLM\n",
        "print_wrapped(no_rag_response.content)"
      ],
      "metadata": {
        "id": "XNofucBefh2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to answer the question using retrieval-augmented generation (RAG)\n",
        "\n",
        "# Invoke the RAG chain with the input question\n",
        "# - The input is a dictionary with the key \"input\" containing the specific question\n",
        "response = rag_chain.invoke({\"input\": \"What did ARLIS announce on 5/28/2024?\"})\n",
        "\n",
        "# Print the content of the response from the RAG chain\n",
        "# - 'response[\"answer\"]' contains the generated answer from the RAG chain\n",
        "print_wrapped(response[\"answer\"])"
      ],
      "metadata": {
        "id": "tu9p_ZS7f0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzuovpAxWoPM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}