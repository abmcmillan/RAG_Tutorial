{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54epNJ9SSLDt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! pip install langchain langchain-openai langchain-community langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# specify OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Insert_Key_Here'"
      ],
      "metadata": {
        "id": "MQ80bWWLSTyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Roster information from the Terp's Men's Basketball Website. Note that this takes >3 minutes, so I have done this for us, but here is Python code where you could download it:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Define the URL of the website from which we want to load documents\n",
        "url = \"https://umterps.com/sports/mens-basketball/roster\"\n",
        "\n",
        "# Import the RecursiveUrlLoader class from the langchain_community.document_loaders.recursive_url_loader module\n",
        "# This class is used to recursively load documents from a given URL\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# Import the BeautifulSoup class from the bs4 module and alias it as Soup\n",
        "# BeautifulSoup is a library used for parsing HTML and XML documents\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "\n",
        "# Define a function to clean up the extracted HTML text\n",
        "# - This function takes the raw HTML content and processes it to extract clean text\n",
        "def clean_html_text(html):\n",
        "    # Parse the HTML content with BeautifulSoup\n",
        "    soup = Soup(html, \"html.parser\")\n",
        "    # Get all text, stripped of leading/trailing whitespace and extra newlines/tabs\n",
        "    cleaned_text = soup.get_text(separator=' ', strip=True)\n",
        "    return cleaned_text\n",
        "\n",
        "# Create an instance of RecursiveUrlLoader\n",
        "# - url: the starting URL to load documents from\n",
        "# - max_depth: the maximum depth to which links on the page should be followed (here, 2 levels deep)\n",
        "# - extractor: a lambda function that takes the HTML content, parses it with BeautifulSoup, and cleans the text\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=2, extractor=lambda x: clean_html_text(x)\n",
        ")\n",
        "\n",
        "# Load the documents from the specified URL and its linked pages up to the specified depth\n",
        "# The loaded and cleaned content is stored in the 'docs' variable\n",
        "docs = loader.load()\n",
        "\n",
        "# 'docs' now contains the cleaned text content of the pages loaded from the URL and its linked pages up to the specified depth\n",
        "\n",
        "# Import the pickle module, which is used for serializing and deserializing Python objects\n",
        "import pickle\n",
        "\n",
        "# Open a file named 'terps_mens_basketball_roster_info.pkl' in write-binary mode\n",
        "# 'wb' stands for write-binary\n",
        "with open('terps_mens_basketball_roster_info_cleaned.pkl', 'wb') as file:\n",
        "    # Use the pickle.dump() function to serialize the 'docs' object and write it to the file\n",
        "    pickle.dump(docs, file)\n",
        "```"
      ],
      "metadata": {
        "id": "sGqgYwJ1mViH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the terps_mens_basketball_roster_info.pkl file as docs\n",
        "! wget https://github.com/abmcmillan/RAG_Tutorial/raw/main/terps_mens_basketball_roster_info_cleaned.pkl\n",
        "\n",
        "# Import the pickle module, which is used for serializing and deserializing Python objects\n",
        "import pickle\n",
        "\n",
        "# Open a file named 'terps_mens_basketball_roster_info.pkl' in write-binary mode\n",
        "# 'wb' stands for write-binary\n",
        "with open('terps_mens_basketball_roster_info_cleaned.pkl', 'rb') as file:\n",
        "    # Use the pickle.dump() function to deserialize the 'docs' object and read from the file\n",
        "    docs = pickle.load(file)"
      ],
      "metadata": {
        "id": "zy_jcD3_WRny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RecursiveCharacterTextSplitter class from the langchain_text_splitters module\n",
        "# This class is used to split text into chunks based on character count\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Create an instance of RecursiveCharacterTextSplitter\n",
        "# - chunk_size: the desired size of each text chunk (here, 2000 characters)\n",
        "# - chunk_overlap: the number of characters that overlap between consecutive chunks (here, 300 characters)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)\n",
        "\n",
        "# Use the text_splitter instance to split the 'docs' text into chunks\n",
        "# The split_documents method will take the 'docs' object (a collection of text documents) and split it\n",
        "# into smaller chunks according to the specified chunk size and overlap\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 'splits' now contains the text documents divided into smaller chunks of approximately 1000 characters each,\n",
        "# with an overlap of 200 characters between consecutive chunks"
      ],
      "metadata": {
        "id": "dIpZaBGlT7Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Chroma class from the langchain_chroma module\n",
        "# Chroma is a vector database that stores and retrieves documents based on their vector embeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Import the OpenAIEmbeddings class from the langchain_openai module\n",
        "# OpenAIEmbeddings is used to compute embeddings for documents using OpenAI's embedding model\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Create a vector database (Chroma) from the split documents\n",
        "# - documents: the list of document chunks that need to be stored in the vector database\n",
        "# - embedding: an instance of OpenAIEmbeddings used to compute vector embeddings for the documents\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "\n",
        "# Create a retriever from the vector database\n",
        "# The retriever can be used to search for and retrieve documents based on their vector embeddings\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 'vectorstore' now contains the vector embeddings for the document chunks and allows for efficient\n",
        "# similarity-based retrieval of documents\n",
        "# 'retriever' is an interface to query the vector database and fetch relevant documents based on query embeddings"
      ],
      "metadata": {
        "id": "GOW6c8EbT9RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system prompt for the Retrieval-Augmented Generation (RAG) model\n",
        "# The system prompt provides instructions for the assistant on how to answer questions\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks about the University of Maryland\"\n",
        "    \"Terrapins Men's Basketball team. Use the following pieces of retrieved context to \"\n",
        "    \"answer the question. If you don't know the answer, say that you don't know. Use \"\n",
        "    \"three sentences maximum and keep the answer concise.\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# Import the ChatPromptTemplate class from the langchain_core.prompts module\n",
        "# ChatPromptTemplate is used to create a structured template for chat-based prompts\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create a ChatPromptTemplate instance using the system and human messages\n",
        "# - (\"system\", system_prompt): The system message provides the assistant with instructions\n",
        "# - (\"human\", \"{input}\"): The human message represents the user's input question, which will be filled in dynamically\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 'prompt' is now a structured template for generating responses using the RAG model\n",
        "# The system message guides the assistant on how to use the retrieved context to answer questions concisely\n",
        "# The human message represents the user's input question to which the assistant will respond"
      ],
      "metadata": {
        "id": "hNWLhZ6SUoYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the ChatOpenAI class from the langchain_openai module\n",
        "# ChatOpenAI is used to define and interact with OpenAI's chat-based language models\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create an instance of ChatOpenAI\n",
        "# - model: specifies the model version to be used, in this case \"gpt-3.5-turbo-0125\"\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n",
        "\n",
        "# Import the create_stuff_documents_chain function from the langchain.chains.combine_documents module\n",
        "# This function is used to create a chain that processes documents and generates answers\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Create a question-answer chain using the language model (llm) and the defined prompt template\n",
        "# - llm: the language model to be used for generating answers\n",
        "# - prompt: the structured prompt template for generating responses\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# Import the create_retrieval_chain function from the langchain.chains module\n",
        "# This function is used to create a retrieval chain that combines document retrieval and question answering\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# Create a retrieval-augmented generation (RAG) chain\n",
        "# - retriever: the document retriever that fetches relevant documents based on query embeddings\n",
        "# - question_answer_chain: the question-answer chain that generates answers using the retrieved documents\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "# 'llm' is the language model instance used for generating answers\n",
        "# 'question_answer_chain' is the chain that processes documents and generates answers using the language model and prompt template\n",
        "# 'rag_chain' is the combined chain that first retrieves relevant documents and then generates answers based on those documents\n"
      ],
      "metadata": {
        "id": "kqv4WbYBUzxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a utility function to wrap print and wrap text for Colab\n",
        "\n",
        "# Import the textwrap module from the standard library\n",
        "# textwrap is used to wrap text into paragraphs with a specified width\n",
        "import textwrap\n",
        "\n",
        "# Print the response with text wrapping for better display\n",
        "# - textwrap.fill: Wraps the input text to the specified width (50 characters in this case)\n",
        "def print_wrapped( text, width=50 ):\n",
        "  print(textwrap.fill( text, width=width))"
      ],
      "metadata": {
        "id": "l7co3SlP4bj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the language model (LLM) directly without retrieval-augmented generation (RAG)\n",
        "# Invoke the LLM with a system message providing context and a human message containing the user's question\n",
        "no_rag_response = llm.invoke([\n",
        "    # System message: Provides context to the assistant about its role\n",
        "    (\"system\",\n",
        "     \"You are an assistant for question-answering tasks about the University of Maryland\"\n",
        "     \"Terrapins Men's Basketball team. If you don't know the answer, say that you don't \"\n",
        "     \"know. Use three sentences maximum and keep the answer concise.\\n\\n\"),\n",
        "    # Human message: The user's input question to the assistant\n",
        "    (\"human\", \"Who is the current head coach of the Maryland Terrapins Men's Basketball Team?\"),\n",
        "])\n",
        "\n",
        "print_wrapped( no_rag_response.content )"
      ],
      "metadata": {
        "id": "zFCYLMeQU3-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use retrieval-augmented generation (RAG) to answer the question\n",
        "# Invoke the RAG chain with the user's question as input\n",
        "response = rag_chain.invoke({\"input\": \"Who is the current head coach of the Maryland Terrapins Men's Basketball Team?\"})\n",
        "\n",
        "# Print the response with text wrapping for better display\n",
        "# - textwrap.fill: Wraps the answer text to the specified width (50 characters in this case)\n",
        "print_wrapped(response[\"answer\"] )"
      ],
      "metadata": {
        "id": "6IyrCk3UU5lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"Where did Kevin Willard play college basketball and what position did he play?\"})\n",
        "print_wrapped(response[\"answer\"] )"
      ],
      "metadata": {
        "id": "NjkPmg8sVWFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impressive, but how do we know this is accurate? Can we get a source?\n",
        "# indeed we can, Langchain provides context for nearest searches\n",
        "response[\"context\"][0]"
      ],
      "metadata": {
        "id": "PKA57UDHVezU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"Who is the shortest player on the Terps Men's Basketball Team?\"})\n",
        "print_wrapped(response[\"answer\"] )"
      ],
      "metadata": {
        "id": "uFVkeop_jBcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"Who is the biggest jokester on the team?\"})\n",
        "print_wrapped(response[\"answer\"] )"
      ],
      "metadata": {
        "id": "xlxrdU6tjcLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"How short is Julian Reese?\"})\n",
        "print_wrapped(response[\"answer\"] )"
      ],
      "metadata": {
        "id": "PFzr1Ia6kEte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"How much does Reese weigh?\"})\n",
        "print_wrapped(response[\"answer\"], width=50 )"
      ],
      "metadata": {
        "id": "t2TFTfxlkS03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"What players are not from the United States?\"})\n",
        "print_wrapped(response[\"answer\"], width=50 )"
      ],
      "metadata": {
        "id": "LWJCPOLV3tSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\"input\": \"Who is the video coordinator and how many years of experience do they have? \"})\n",
        "print_wrapped(response[\"answer\"], width=50 )"
      ],
      "metadata": {
        "id": "fInw7o-PKZoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"context\"]"
      ],
      "metadata": {
        "id": "i7SFO2jLK62b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6a50Tq6pDVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}